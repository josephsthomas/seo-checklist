[
  {
    "bug_id": "R07-B-001",
    "severity": "high",
    "category": "Missing Disclaimer",
    "component": "ReadabilityShareView",
    "file_line": "src/components/readability/ReadabilityShareView.jsx:276-308",
    "description": "Public shared analysis view displays LLM coverage data (model names, content/headings/entities coverage percentages, usefulness scores) without any AI-generated content disclaimer. The AIDisclaimer component exists in the codebase but is not imported or rendered on this publicly accessible page.",
    "steps": "1. Generate a readability analysis for any URL.\n2. Create a share link from the dashboard.\n3. Open the share link in a browser (no auth required).\n4. Scroll to the 'How AI Models See This Content' section.\n5. Observe that LLM coverage data is displayed with no AI disclaimer.",
    "expected": "The shared analysis view should include an AI disclaimer (using the AIDisclaimer or AIDisclaimerInline component) near the LLM coverage table and AI Visibility Summary sections, informing viewers that these are AI-generated assessments that may contain inaccuracies.",
    "actual": "LLM coverage percentages and usefulness scores are displayed as authoritative data on a publicly accessible page with zero AI disclaimer or caveat. The 'About This Report' section (line 349-354) mentions AI but does not disclaim that scores are AI-generated estimates.",
    "impact": "Public-facing users may treat AI-generated coverage percentages and usefulness scores as factual/verified data, increasing hallucination risk and potential liability. This contradicts the app's own AI policy pattern established by AIDisclaimer.jsx."
  },
  {
    "bug_id": "R07-B-002",
    "severity": "high",
    "category": "Missing Disclaimer",
    "component": "ReadabilityDashboard",
    "file_line": "src/components/readability/ReadabilityDashboard.jsx:300-311",
    "description": "The AI Visibility Summary section renders AI-generated content summary text (from aiAssessment.contentSummary) directly without any AI disclaimer. The dashboard also renders an executive summary (lines 306-310) that is algorithmically generated based on score thresholds but presented as authoritative analysis.",
    "steps": "1. Run a readability analysis on any URL.\n2. View the results dashboard.\n3. Observe the teal-colored 'AI Visibility Summary' section below the score card.\n4. Note that no AI disclaimer is present on the entire dashboard page.",
    "expected": "The dashboard should include the AIDisclaimer component (compact or inline variant) near AI-generated sections such as the AI Visibility Summary, citation worthiness score, and the LLM Preview tab, clearly labeling these as AI-generated assessments.",
    "actual": "The AI-generated content summary is rendered as a plain paragraph with no indication it was AI-generated. The AIDisclaimer component exists (src/components/shared/AIDisclaimer.jsx) but is never imported or used in the dashboard.",
    "impact": "Users may take AI-generated visibility summaries and citation worthiness scores at face value without understanding they are estimates that may contain hallucinated or inaccurate assessments."
  },
  {
    "bug_id": "R07-B-003",
    "severity": "medium",
    "category": "Model Selection",
    "component": "ReadabilityLLMPreview",
    "file_line": "src/components/readability/ReadabilityLLMPreview.jsx:21-25",
    "description": "LLM model versions are hardcoded as static constants: 'Claude 3.5 Sonnet', 'GPT-4o', 'Gemini 1.5 Pro'. These do not match the model versions referenced in ReadabilityProcessingScreen.jsx (lines 22-24) which lists 'Claude Sonnet 4.5', 'GPT-4o', 'Gemini 2.0 Flash'. This inconsistency means model names shown in the preview differ from those used during processing.",
    "steps": "1. Start a readability analysis and observe the processing screen.\n2. Note the model names shown: 'Claude Sonnet 4.5', 'GPT-4o', 'Gemini 2.0 Flash'.\n3. After analysis completes, switch to the 'How AI Sees Your Content' tab.\n4. Note the model names shown: 'Claude 3.5 Sonnet', 'GPT-4o', 'Gemini 1.5 Pro'.\n5. Observe the mismatch between processing and preview screens.",
    "expected": "Model names and versions should be consistent across all components and should be derived from a single source of truth (e.g., a shared configuration), and ideally fetched dynamically or at minimum kept in sync.",
    "actual": "Three different components hardcode different model version strings: ReadabilityLLMPreview uses 'Claude 3.5 Sonnet' and 'Gemini 1.5 Pro', ReadabilityProcessingScreen uses 'Claude Sonnet 4.5' and 'Gemini 2.0 Flash', and ReadabilityPDFPreview (line 78) uses only generic names like 'Claude', 'OpenAI GPT', 'Google Gemini'.",
    "impact": "Users see different model versions depending on which screen they view, creating confusion about which AI models are actually being used for analysis and undermining trust in the tool's accuracy."
  },
  {
    "bug_id": "R07-B-004",
    "severity": "medium",
    "category": "Confidence Gap",
    "component": "ReadabilityLLMColumn",
    "file_line": "src/components/readability/ReadabilityLLMColumn.jsx:238-272",
    "description": "The usefulness score section displays a raw numeric score (out of 10) from AI model extraction without any confidence interval, margin of error, or reliability indicator. The score is presented as a precise metric via a visual bar chart (10 colored squares) but has no indication of its confidence level or how it was derived.",
    "steps": "1. Run a readability analysis on any URL.\n2. Navigate to the 'How AI Sees Your Content' tab.\n3. Expand a specific LLM column (e.g., Claude).\n4. View the 'Usefulness Score' section.\n5. Observe the score is displayed as a precise X/10 value with colored bars.",
    "expected": "AI-generated usefulness scores should include confidence indicators (e.g., 'high/medium/low confidence'), a margin of error, or at minimum a tooltip explaining that this is an AI estimate that may vary between runs.",
    "actual": "The usefulness score is presented as a precise numeric value with visual emphasis (colored progress bars) but zero confidence metadata. The usefulnessExplanation field (line 266-270) is only shown if provided by the backend, with no fallback to explain the score's reliability.",
    "impact": "Users may over-rely on AI-generated usefulness scores for important content decisions without understanding the inherent uncertainty in these AI-derived metrics."
  },
  {
    "bug_id": "R07-B-005",
    "severity": "medium",
    "category": "Confidence Gap",
    "component": "ReadabilityScoreCard",
    "file_line": "src/components/readability/ReadabilityScoreCard.jsx:226-243",
    "description": "The Citation Likelihood score (citationWorthiness) is displayed as a precise number out of 100 with no confidence interval or methodology explanation. This AI-generated metric is presented alongside the overall readability score, giving it similar perceived authority, but there is no indication of how reliable this prediction is.",
    "steps": "1. Run a readability analysis on a URL that triggers citation worthiness assessment.\n2. View the results score card.\n3. Observe the 'Citation Likelihood' section showing 'X/100'.\n4. Note the descriptor 'How likely this content is to be quoted in AI answers'.",
    "expected": "The citation likelihood score should include a confidence level (e.g., 'estimated', 'approximate'), a range rather than a single number, or a disclaimer that this is a predictive AI metric with inherent uncertainty.",
    "actual": "The score is rendered as a single precise number (e.g., '72/100') with the claim that it represents how likely content is to be quoted in AI answers. No confidence interval, margin of error, or 'estimated' qualifier is provided.",
    "impact": "Users may make strategic content decisions based on an AI-predicted citation likelihood presented with false precision. An inaccurate high or low score could lead to misallocated content optimization efforts."
  },
  {
    "bug_id": "R07-B-006",
    "severity": "high",
    "category": "Output Validation",
    "component": "ReadabilityLLMColumn",
    "file_line": "src/components/readability/ReadabilityLLMColumn.jsx:166-215",
    "description": "LLM extraction data (title, description, primaryTopic, mainContent, entities) is rendered directly into the DOM without any sanitization or output validation. The extraction.title, extraction.description, extraction.primaryTopic, and extraction.mainContent values are rendered using plain JSX text interpolation, but the entity rendering (line 210) uses toString() on arbitrary objects, and mainContent is rendered in a pre-wrap whitespace div.",
    "steps": "1. Analyze a URL whose content contains special characters or HTML entities.\n2. Navigate to the LLM preview tab.\n3. Examine the extracted title, description, entities, and main content.\n4. Note that values are rendered directly without sanitization checks.",
    "expected": "All AI-generated extraction data should be validated before rendering: check for expected types, sanitize against XSS if any HTML content is present, validate that scores are within expected ranges, and handle malformed entity objects gracefully.",
    "actual": "The component renders AI extraction data directly. Entity rendering (line 210) uses a fallback chain (entity.name || entity.text || String(entity)) that could produce '[object Object]' or unexpected output. The mainContent field (line 196) is rendered as raw text with whitespace preservation but no length or content validation.",
    "impact": "Malformed or adversarial AI extraction output could result in XSS vulnerabilities (if HTML makes it through), rendering errors, or display of nonsensical content that undermines user trust."
  },
  {
    "bug_id": "R07-B-007",
    "severity": "medium",
    "category": "AI Error Handling",
    "component": "ReadabilityProcessingScreen",
    "file_line": "src/components/readability/ReadabilityProcessingScreen.jsx:30-38",
    "description": "The FACTOIDS array contains unverified statistical claims presented as educational facts during AI analysis processing. Claims like 'Structured data (JSON-LD) can improve how AI models understand and cite your content by up to 40%' and 'Pages with clear heading hierarchies are 3x more likely to be cited in AI-generated answers' are presented without sources or caveats.",
    "steps": "1. Start a readability analysis on any URL.\n2. Observe the processing screen 'Did you know?' section.\n3. Wait for factoids to rotate (every 8 seconds).\n4. Read the statistical claims (e.g., 'up to 40%', '3x more likely').",
    "expected": "Statistical claims about AI behavior should either be sourced, marked as approximate/estimated, or clearly labeled as general guidance rather than verified facts. Each factoid should include a qualifier like 'research suggests' or 'based on industry analysis'.",
    "actual": "Specific numerical claims are stated as facts without sources, qualifiers, or any indication that these are estimates. This is a hallucination risk because users may cite these statistics in their own content strategies.",
    "impact": "Users may reference these unsourced AI-related statistics in reports or presentations, spreading potentially inaccurate claims. This undermines the tool's credibility if the statistics cannot be verified."
  },
  {
    "bug_id": "R07-B-008",
    "severity": "medium",
    "category": "Missing Disclaimer",
    "component": "ReadabilityPDFPreview",
    "file_line": "src/components/readability/ReadabilityPDFPreview.jsx:355-357",
    "description": "The PDF report's LLM Summary page includes only a minimal, easily-overlooked italic footnote as its AI disclaimer: 'LLM previews show how AI models interpret content when provided to them.' This text is styled as 7px gray italic text at the bottom, making it functionally invisible in printed/exported PDFs.",
    "steps": "1. Run a readability analysis.\n2. Click Export â†’ PDF.\n3. Enable the 'LLM Summary' toggle.\n4. Navigate to page 5 in the preview.\n5. Observe the disclaimer text at the bottom.",
    "expected": "The PDF report should include a prominent AI disclaimer (matching the severity used in the web UI's AIDisclaimer.jsx component) on any page containing AI-generated data, including the LLM Summary page. The disclaimer should be clearly visible in the exported PDF, not rendered as 7px italic text.",
    "actual": "The only disclaimer is a single line in 7px italic gray text: 'LLM previews show how AI models interpret content when provided to them.' This does not warn about potential inaccuracies, does not mention hallucination risk, and would be barely visible in a printed PDF.",
    "impact": "PDF exports with AI-generated LLM data may be shared with stakeholders who see usefulness scores and model comparisons without understanding these are AI estimates. The exported PDF lacks the AI liability protections present in other parts of the application."
  },
  {
    "bug_id": "R07-B-009",
    "severity": "high",
    "category": "Missing Disclaimer",
    "component": "ReadabilityRecommendations",
    "file_line": "src/components/readability/ReadabilityRecommendations.jsx:42-59",
    "description": "AI-sourced recommendations from aiAssessment.readabilityIssues are merged into the general recommendations list with only a small 'AI Suggested' badge on each card and a minimal footnote at the bottom (lines 191-198). The recommendations are mixed with rule-based recommendations without clear visual separation, and the AI-generated content (title, description, priority, effort, impact) is used directly from the AI assessment without output validation.",
    "steps": "1. Run a readability analysis that generates AI assessment data.\n2. Navigate to the Recommendations tab.\n3. Observe that AI-sourced recommendations are intermixed with rule-based ones.\n4. Note the small purple 'AI Suggested' badge is the only distinguishing marker.\n5. Check the bottom footnote about AI limitations.",
    "expected": "AI-generated recommendations should be (1) visually separated or grouped distinctly from rule-based recommendations, (2) validated for reasonable content before display, and (3) accompanied by the AIDisclaimer component rather than just a subtle footnote. AI recommendation fields (title, description, priority) should be validated against expected types and lengths.",
    "actual": "AI recommendations are merged into the same list as rule-based ones with only a tiny badge distinguishing them. The AI-generated fields (issue.title, issue.description, issue.suggestion, issue.priority, issue.effort, issue.impact) are used without type checking or content validation. Default fallbacks (lines 49-54) may produce empty or misleading recommendations.",
    "impact": "Users may implement AI-generated recommendations without realizing they are less reliable than rule-based checks. Unvalidated AI output could include hallucinated suggestions that harm content strategy."
  },
  {
    "bug_id": "R07-B-010",
    "severity": "medium",
    "category": "AI Error Handling",
    "component": "SchemaGeneratorPage",
    "file_line": "src/components/schema-generator/SchemaGeneratorPage.jsx:41-68",
    "description": "The schema generator calls generateSchema() (an AI-powered function) with minimal error handling. The catch block (line 65-68) only displays the raw error message to the user via toast and error view. There is no AI-specific error handling such as retry logic, partial result recovery, fallback to non-AI generation, or user guidance about what went wrong with the AI generation.",
    "steps": "1. Navigate to the Schema Generator.\n2. Submit HTML content for schema generation.\n3. Observe the processing screen.\n4. If AI generation fails (e.g., API timeout, rate limit), the error view shows only a generic error message.\n5. The only option is 'Try Again' with no guidance.",
    "expected": "AI-powered schema generation should include: (1) specific error messages for different AI failure modes (rate limit, timeout, content too long, model unavailable), (2) automatic retry with exponential backoff, (3) fallback to rule-based schema generation if AI fails, (4) partial result preservation if generation partially completed.",
    "actual": "The error handler catches all errors uniformly, displays err.message as-is, and offers only a 'Try Again' button. No distinction between AI-specific errors and other errors. The confidence field (line 56) from schemaResults is stored but never displayed to users.",
    "impact": "Users receive unhelpful error messages when AI generation fails, with no way to recover partial results or understand why generation failed. The stored confidence score is never surfaced to help users assess schema quality."
  },
  {
    "bug_id": "R07-B-011",
    "severity": "low",
    "category": "Missing Disclaimer",
    "component": "SchemaProcessingScreen",
    "file_line": "src/components/schema-generator/upload/SchemaProcessingScreen.jsx:50-78",
    "description": "The schema processing screen displays a step labeled 'Analyzing with Claude AI' (line 50) and a tip that says 'AI generates production-ready JSON-LD' (line 79) without disclaiming that AI-generated schemas require human validation. Calling AI output 'production-ready' is misleading and contradicts the app's own AI policy of requiring review before use.",
    "steps": "1. Navigate to the Schema Generator.\n2. Upload or paste HTML content.\n3. Observe the processing screen.\n4. Read step 4: 'Analyzing with Claude AI'.\n5. Read the tip at the bottom: 'AI generates production-ready JSON-LD with required and recommended properties'.",
    "expected": "The tip should not claim AI output is 'production-ready'. It should instead say something like 'AI generates JSON-LD suggestions that should be reviewed before use' to be consistent with the AIDisclaimer component's messaging. The processing step should note that AI analysis may require review.",
    "actual": "The tip explicitly claims 'AI generates production-ready JSON-LD', implying the output can be used without review. This directly contradicts the AIExportConfirmation component which requires users to acknowledge that 'AI-generated content may contain errors, inaccuracies, or inappropriate suggestions'.",
    "impact": "Users may deploy AI-generated schema markup directly to production without review, potentially introducing schema errors that could hurt SEO or trigger structured data penalties from search engines."
  },
  {
    "bug_id": "R07-B-012",
    "severity": "medium",
    "category": "Output Validation",
    "component": "ReadabilityShareView",
    "file_line": "src/components/readability/ReadabilityShareView.jsx:37-47",
    "description": "The getGradeFromScore function in the share view uses different grade boundaries than those used in the main application's gradeMapper utility (imported in ReadabilityScoreCard.jsx line 3). This means the same score could receive different letter grades depending on whether viewed in the authenticated dashboard vs. the public share view.",
    "steps": "1. Run an analysis that produces a score of exactly 75.\n2. View the result on the dashboard (uses getGrade from gradeMapper utility).\n3. Generate a share link and view the shared analysis.\n4. Compare the letter grade shown on each view.\n5. Observe potential discrepancy in grade assignment.",
    "expected": "Grade assignment should use a single shared utility function across all views to ensure consistent grading. The ReadabilityShareView should import and use the same getGrade/getGradeFromScore function as ReadabilityScoreCard.",
    "actual": "ReadabilityShareView defines its own getGradeFromScore function with thresholds at 95/90/85/80/75/70/65/60 producing grades A+/A/A-/B+/B/C/C-/D/F. The main gradeMapper utility (used by ReadabilityScoreCard) likely uses different thresholds, leading to grade inconsistency between views.",
    "impact": "A client receiving a shared report link may see a different grade than what the analyst sees on the dashboard, creating confusion and undermining trust in the scoring methodology."
  },
  {
    "bug_id": "R07-B-013",
    "severity": "medium",
    "category": "Hallucination Risk",
    "component": "ReadabilityLLMDiff",
    "file_line": "src/components/readability/ReadabilityLLMDiff.jsx:12-32",
    "description": "The LLM diff algorithm uses a naive word-level set comparison (computeWordDiff) that uses Set membership to determine 'common' vs 'removed' words. This approach loses word order and frequency, meaning a word that appears once in LLM-A and five times in LLM-B would show as 'common'. The overlap percentage calculation (line 53) can produce misleading metrics that suggest more or less agreement between LLMs than actually exists.",
    "steps": "1. Run a readability analysis.\n2. Navigate to the 'How AI Sees Your Content' tab.\n3. Switch to 'Diff' view mode.\n4. Compare two LLMs on a content page with repeated words.\n5. Observe the 'overlap percentage' metric and highlighted words.\n6. Note that word frequency is ignored in the comparison.",
    "expected": "The diff algorithm should account for word order and frequency to provide an accurate representation of how similarly two LLMs interpret content. The overlap percentage should be clearly labeled as approximate and the methodology should be explained.",
    "actual": "The set-based comparison considers only unique words, ignoring order and frequency. This can produce artificially high overlap percentages (e.g., two very different passages with common English words would show high overlap) or miss meaningful differences in how LLMs structure their extractions.",
    "impact": "Users relying on the diff view to understand AI model disagreements may get a false sense of consensus or difference, leading to incorrect conclusions about content readability across AI models."
  },
  {
    "bug_id": "R07-B-014",
    "severity": "high",
    "category": "AI Error Handling",
    "component": "ReadabilityPage",
    "file_line": "src/components/readability/ReadabilityPage.jsx:117-139",
    "description": "The three analysis handler functions (handleAnalyzeUrl, handleAnalyzeHtml, handleAnalyzePaste) all catch errors silently with empty catch blocks containing only a comment '// Error is handled in hook'. This pattern means if the hook's error handling fails or is incomplete, errors are silently swallowed. There is no AI-specific error categorization or user guidance for different failure modes.",
    "steps": "1. Trigger an analysis that fails (e.g., unreachable URL, API rate limit).\n2. Observe the error banner at lines 228-248.\n3. Note the error message is displayed generically without categorization.\n4. Note there is no retry mechanism or alternative suggestion.\n5. If the hook fails to set the error state, the error is silently swallowed.",
    "expected": "Error handlers should: (1) not silently swallow errors, (2) categorize AI-specific errors (rate limit, timeout, token limit exceeded, model unavailable) differently from network/input errors, (3) provide specific recovery guidance based on error type, (4) log errors for debugging even in production.",
    "actual": "All three handlers use identical empty catch blocks that rely entirely on the hook to handle errors. The error banner (lines 228-248) shows a single generic message with no error categorization, no retry button, and no AI-specific guidance. The only recovery option is 'Try another analysis'.",
    "impact": "When AI model calls fail, users receive generic unhelpful error messages with no way to understand the failure cause or take corrective action. Silent error swallowing could mask critical AI service outages."
  },
  {
    "bug_id": "R07-B-015",
    "severity": "low",
    "category": "Token Management",
    "component": "ReadabilityInputScreen",
    "file_line": "src/components/readability/ReadabilityInputScreen.jsx:222-226",
    "description": "The paste input tab limits content to 2MB by raw byte size but has no consideration for token limits of the AI models that will process the content. A 2MB HTML file could contain hundreds of thousands of tokens, potentially exceeding the context window of models like GPT-4o or Claude, leading to truncated analysis or unexpected costs.",
    "steps": "1. Navigate to the Readability Checker.\n2. Switch to the 'Paste HTML' tab.\n3. Paste a large HTML document approaching 2MB.\n4. Observe only the byte size limit is enforced.\n5. Submit for analysis.",
    "expected": "The input screen should provide guidance on approximate token count based on content length, warn users when content may exceed AI model context windows, or automatically indicate that very large content will be truncated or chunked during analysis.",
    "actual": "Only a raw byte-size limit (2MB) is enforced. There is no token count estimate, no warning about potential AI model context window limitations, and no indication of how large content will be handled during multi-model analysis.",
    "impact": "Users may submit content that exceeds AI model token limits, resulting in silently truncated analysis, incomplete LLM extractions, or unexpected API costs from processing oversized content."
  },
  {
    "bug_id": "R07-B-016",
    "severity": "medium",
    "category": "Missing Disclaimer",
    "component": "ScheduledReportsPanel",
    "file_line": "src/components/reports/ScheduledReportsPanel.jsx:75-79",
    "description": "The Image Alt Text Audit report type includes an 'includeAIsuggestions' configuration field (line 79) that enables AI-generated alt text suggestions in automated scheduled reports. These AI suggestions would be automatically generated and emailed to recipients without any AI disclaimer infrastructure in the scheduled report delivery system.",
    "steps": "1. Navigate to the Scheduled Reports panel.\n2. Create a new schedule for 'Image Alt Text Audit'.\n3. Note the 'includeAIsuggestions' field is available.\n4. Enable AI suggestions and schedule the report.\n5. Observe that no AI disclaimer is attached to scheduled report deliveries.",
    "expected": "Scheduled reports that include AI-generated suggestions should automatically include an AI disclaimer in the report output, email body, or report header. The scheduling form should warn users that AI suggestions in automated reports may contain inaccuracies.",
    "actual": "The 'includeAIsuggestions' option is offered without any disclaimer about AI limitations. The runNow function (lines 224-233) simulates report generation with a simple setTimeout and toast message, with no AI processing validation or disclaimer injection into report output.",
    "impact": "Automated reports with AI-generated alt text suggestions may be sent to stakeholders who implement them without review, potentially introducing inaccurate or inappropriate alt text descriptions across a website."
  }
]
