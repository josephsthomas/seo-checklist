[
  {
    "bug_id": "R09-C-001",
    "severity": "CRITICAL",
    "category": "Environment Issue",
    "component": "Firebase Configuration",
    "file_line": "src/lib/firebase.js:7",
    "description": "Firebase initialization has no validation or fallback when required environment variables are missing. If any VITE_FIREBASE_* variable is undefined, initializeApp() will be called with undefined values, producing a cryptic Firebase error at runtime rather than a clear startup diagnostic. Unlike the AI services which check for config presence, firebase.js blindly passes import.meta.env values without any guard.",
    "steps": "1. Remove or leave blank any VITE_FIREBASE_* variable from the .env file. 2. Run 'npm run build && npm run preview'. 3. Open the application in a browser.",
    "expected": "The application should detect missing Firebase configuration at startup and display a clear error message indicating which environment variables are missing, preventing a confusing runtime crash.",
    "actual": "Firebase SDK throws an opaque 'FirebaseError: Firebase: Error (auth/invalid-api-key)' or similar runtime error that is difficult to diagnose, especially in production where the .env file may not be properly configured during deployment.",
    "impact": "Complete application failure in any deployment environment where Firebase variables are not configured. New developers and CI/CD pipelines may waste significant time debugging. In production, the entire application is non-functional with no actionable error."
  },
  {
    "bug_id": "R09-C-002",
    "severity": "CRITICAL",
    "category": "Deploy Risk",
    "component": "AI Suggestion Service",
    "file_line": "src/lib/ai/suggestionService.js:17",
    "description": "The VITE_CLAUDE_API_KEY environment variable is exposed in client-side JavaScript bundles. While the suggestionService.js blocks direct API access in production (line 26-31), the schemaGeneratorService.js (line 32) and imageAltService.js (line 32) and aiSuggestionService.js (line 19) do NOT block direct API key usage in production builds. If VITE_CLAUDE_API_KEY is set during a production build, these services will embed and use the API key directly from the browser, exposing it in network requests and the JS bundle.",
    "steps": "1. Set VITE_CLAUDE_API_KEY in .env alongside VITE_AI_PROXY_URL. 2. Run 'npm run build'. 3. Inspect the built JS chunks for the API key string. 4. In schemaGeneratorService.js, imageAltService.js, and aiSuggestionService.js, note that getApiConfig() returns the apiKey without blocking production usage.",
    "expected": "All AI service modules should consistently block direct API key usage in production builds, as suggestionService.js does. The API key should never be embedded in production bundles.",
    "actual": "schemaGeneratorService.js, imageAltService.js, and metaGeneratorService.js allow direct API key usage in production without the production guard that exists in suggestionService.js. The aiSuggestionService.js only logs a console.warn but does not block it.",
    "impact": "API key exposure in production bundles creates a critical security risk. Malicious actors can extract the key from browser DevTools and incur unlimited API charges or abuse the key for unauthorized purposes."
  },
  {
    "bug_id": "R09-C-003",
    "severity": "HIGH",
    "category": "Bundle Size",
    "component": "Unified Export Service",
    "file_line": "src/lib/unifiedExportService.js:6",
    "description": "unifiedExportService.js statically imports jsPDF, jspdf-autotable, ExcelJS, and JSZip at the top level (lines 6-9). These are very large libraries (jsPDF ~300KB, ExcelJS ~1MB, JSZip ~100KB minified). Unlike useReadabilityExport.js which uses dynamic import() for jsPDF (line 97), unifiedExportService.js forces all these heavy libraries into the main bundle or a shared chunk, defeating the manual chunk splitting configured in vite.config.js.",
    "steps": "1. Run 'npm run build'. 2. Analyze the output bundle sizes. 3. Note that jsPDF, ExcelJS, and JSZip are included in whatever chunk imports unifiedExportService.js. 4. Compare with useReadabilityExport.js which dynamically imports jsPDF.",
    "expected": "Heavy export libraries (jsPDF, ExcelJS, JSZip) should be dynamically imported only when export functionality is actually used, keeping the initial bundle small.",
    "actual": "Static imports of jsPDF, ExcelJS, and JSZip in unifiedExportService.js cause these large libraries to be bundled eagerly. Any component importing unifiedExportService pulls in ~1.4MB of library code regardless of whether export is used.",
    "impact": "Significantly increased initial page load time and bundle size. Users who never use export features still download over 1MB of unnecessary JavaScript, degrading Core Web Vitals (LCP, TBT) and mobile performance."
  },
  {
    "bug_id": "R09-C-004",
    "severity": "HIGH",
    "category": "Bundle Size",
    "component": "PDF Generator",
    "file_line": "src/lib/pdfGenerator.js:7",
    "description": "pdfGenerator.js statically imports jsPDF and jspdf-autotable at the top level (lines 7-8), similar to unifiedExportService.js. Additionally, excelExport.js (line 1) statically imports ExcelJS. These static imports cause the vendor libraries to be included in eagerly-loaded chunks even though they are only needed when a user explicitly requests a PDF or Excel export.",
    "steps": "1. Run 'npm run build'. 2. Check the generated chunk sizes. 3. Trace which chunks include jsPDF and ExcelJS due to static imports from pdfGenerator.js and excelExport.js.",
    "expected": "Export-related heavy dependencies should use dynamic import() to enable proper code splitting, consistent with the pattern already used in useReadabilityExport.js.",
    "actual": "jsPDF (~300KB) and ExcelJS (~1MB) are statically imported, forcing them into chunks that load before the user ever requests an export.",
    "impact": "Increased bundle sizes hurt page load performance. The vite.config.js manualChunks configuration for vendor-exceljs and vendor-jspdf tries to isolate these, but static imports from multiple entry points may cause them to be loaded unnecessarily early."
  },
  {
    "bug_id": "R09-C-005",
    "severity": "HIGH",
    "category": "Error Logging",
    "component": "Notification Service",
    "file_line": "src/hooks/useNotifications.js:98",
    "description": "The createNotification function (lines 86-101) silently swallows all errors in its catch block with an empty body and a comment 'Silently fail - notification creation is non-critical'. Similarly, markAsRead (line 55) silently swallows errors. While individual notification failures may not be critical, having zero error logging means there is no way to detect systematic Firestore write failures, permission issues, or connectivity problems that could indicate broader infrastructure issues.",
    "steps": "1. Simulate a Firestore write failure (e.g., invalid permissions or network disconnection). 2. Trigger any action that calls createNotification(). 3. Check browser console and any monitoring tools for error traces.",
    "expected": "At minimum, errors should be logged (e.g., console.error) so they appear in browser error monitoring tools. Ideally, failed notifications should be queued for retry or reported to an error tracking service.",
    "actual": "All errors are completely swallowed with no logging, no retry, and no monitoring visibility. Systematic failures go entirely undetected.",
    "impact": "Operations teams cannot diagnose notification delivery failures. If Firestore permissions are misconfigured or the database is degraded, no alerts or logs are generated, leading to prolonged silent failures affecting user experience."
  },
  {
    "bug_id": "R09-C-006",
    "severity": "HIGH",
    "category": "Monitoring Gap",
    "component": "Lazy Load Retry",
    "file_line": "src/utils/lazyWithRetry.js:37",
    "description": "The lazyWithRetry utility only logs chunk loading failures in development mode (line 37: if (import.meta.env.DEV)). In production, when a chunk fails to load after 3 retries, the error is re-thrown as a generic message with no telemetry, no error reporting service call, and no metrics emission. This means production chunk loading failures — which indicate CDN issues, cache invalidation problems, or deployment mismatches — are completely invisible to operations teams.",
    "steps": "1. Deploy the application. 2. Simulate a chunk loading failure (e.g., delete a chunk file from the CDN). 3. Navigate to a lazy-loaded route. 4. Observe that after 3 retries, the error boundary catches the error but no telemetry is sent to any monitoring service.",
    "expected": "Production chunk loading failures should be reported to an error monitoring service (e.g., Sentry, Datadog) with the module name, retry count, and error details so operations can detect and respond to deployment issues.",
    "actual": "In production, chunk load failures are silently re-thrown with only a user-facing message. No telemetry, metrics, or external error reports are generated.",
    "impact": "Deployment failures or CDN issues that cause chunk loading errors go undetected in production. Users see error boundaries but the engineering team has no visibility into the frequency or scope of these failures."
  },
  {
    "bug_id": "R09-C-007",
    "severity": "HIGH",
    "category": "Caching Gap",
    "component": "Analysis Cache",
    "file_line": "src/lib/readability/utils/analysisCache.js:17",
    "description": "The content hash function hashContent() only processes the first 50,000 characters of the HTML content (line 18: Math.min(content.length, 50000)). For large pages exceeding 50KB of HTML, two different pages with identical first 50K characters but different subsequent content will produce the same hash, causing incorrect cache hits and returning stale or wrong analysis results to the user.",
    "steps": "1. Analyze a URL with a page whose HTML exceeds 50,000 characters. 2. Modify content after the 50,000th character (e.g., different footer, different article content). 3. Re-analyze the same URL. 4. Observe that the cache returns the old result because the hash only covers the first 50K characters.",
    "expected": "The hash function should process the entire content or use a sampling strategy that covers both the beginning and end of the document to detect changes anywhere in the content.",
    "actual": "hashContent() truncates at 50,000 characters, creating hash collisions for pages where changes occur beyond that boundary.",
    "impact": "Users receive incorrect cached analysis results for large pages, leading to wrong readability scores and recommendations. This is particularly problematic for e-commerce pages, long-form content, and pages with dynamic content sections near the end."
  },
  {
    "bug_id": "R09-C-008",
    "severity": "MEDIUM",
    "category": "Build Issue",
    "component": "Vite Configuration",
    "file_line": "vite.config.js:25",
    "description": "The Vite build configuration sets chunkSizeWarningLimit to 600KB (line 25), which suppresses warnings for chunks up to 600KB. The default Vite limit is 500KB, and the stated reason is 'since we have lazy loading'. However, the presence of static imports in unifiedExportService.js and pdfGenerator.js means some chunks may exceed even this raised limit. Increasing the warning limit masks genuine bundle size regressions instead of addressing the root cause (missing dynamic imports).",
    "steps": "1. Review vite.config.js build configuration. 2. Run 'npm run build' and check for any chunk size warnings. 3. Note that raising the limit to 600KB hides potential issues rather than solving them.",
    "expected": "The chunk size warning limit should remain at the default (500KB) or lower. Any chunks exceeding the limit should be addressed through proper code splitting and dynamic imports rather than suppressing the warning.",
    "actual": "chunkSizeWarningLimit is raised to 600KB, which suppresses legitimate warnings about oversized chunks that could degrade performance.",
    "impact": "Bundle size regressions go unnoticed during development and CI. Over time, chunk sizes may creep upward without any build-time alert, gradually degrading application load performance."
  },
  {
    "bug_id": "R09-C-009",
    "severity": "MEDIUM",
    "category": "Deploy Risk",
    "component": "Schema Generator Service",
    "file_line": "src/lib/schema-generator/schemaGeneratorService.js:38",
    "description": "Unlike suggestionService.js which throws an error when API key is used in production, schemaGeneratorService.js getApiConfig() (line 30-43) returns { useProxy: false, apiKey } without any production guard. If VITE_CLAUDE_API_KEY is present in the environment during a production build, the service will use direct browser-to-API calls with the header 'anthropic-dangerous-direct-browser-access: true' (line 291). This inconsistency means some AI features are properly secured while others are not.",
    "steps": "1. Set both VITE_AI_PROXY_URL and VITE_CLAUDE_API_KEY in the environment. 2. Remove VITE_AI_PROXY_URL to simulate a misconfiguration. 3. Build for production. 4. Use the schema generator feature. 5. Observe that the API key is sent directly from the browser.",
    "expected": "schemaGeneratorService.js should block direct API key usage in production builds, consistent with the pattern in suggestionService.js.",
    "actual": "schemaGeneratorService.js allows direct API key usage in production without any guard, enabling insecure browser-to-API communication.",
    "impact": "Inconsistent security posture across AI services. A misconfiguration or intentional decision to set VITE_CLAUDE_API_KEY results in API key exposure for schema generation, image alt generation, and meta generation features."
  },
  {
    "bug_id": "R09-C-010",
    "severity": "MEDIUM",
    "category": "Error Logging",
    "component": "Storage Helpers",
    "file_line": "src/utils/storageHelpers.js:48",
    "description": "All error handling in storageHelpers.js relies exclusively on console.error/console.warn calls (lines 48, 64, 68, 75, 90, 314, 362). There is no integration with any structured logging or error monitoring service. Additionally, the importAllData function (line 353) accepts arbitrary JSON data without any schema validation, version checking, or sanitization — it blindly writes to all storage keys, which could corrupt application state if malformed data is imported.",
    "steps": "1. Call importAllData() with a JSON object containing invalid or malformed data for one of the storage keys. 2. Observe that the data is written without validation. 3. Navigate to features that depend on that storage key. 4. Observe broken behavior due to corrupted data.",
    "expected": "importAllData should validate the schema and version of imported data before writing. Error logging should integrate with a monitoring service rather than relying solely on console output.",
    "actual": "importAllData writes arbitrary data to localStorage without validation. All error logging goes only to console, which is not captured by any production monitoring.",
    "impact": "Users can corrupt their local application state by importing malformed backup files. Error conditions in localStorage operations are invisible in production monitoring, making it impossible to diagnose user-reported issues related to data persistence."
  },
  {
    "bug_id": "R09-C-011",
    "severity": "MEDIUM",
    "category": "Monitoring Gap",
    "component": "AI Analyzer",
    "file_line": "src/lib/readability/aiAnalyzer.js:54",
    "description": "In aiAnalyzer.js, when the caller provides an AbortSignal via options.signal (line 54), the code creates a NEW AbortController and its own signal but then uses 'const signal = options.signal || controller.signal'. This means if the caller passes a signal, the internal timeout controller's abort will never trigger because the fetch uses the caller's signal instead. The 120-second timeout (line 10) is effectively bypassed whenever options.signal is provided, which is the normal usage path from useReadabilityAnalysis.",
    "steps": "1. Call analyzeWithAI() from useReadabilityAnalysis which passes options.signal. 2. Make the AI proxy take longer than 120 seconds to respond. 3. Observe that the internal 120s timeout does not abort the request because the caller's signal is used instead.",
    "expected": "The timeout should work in conjunction with the caller's signal. Both should be able to abort the request — the caller for cancellation, the internal timer for timeout protection.",
    "actual": "The internal timeout controller is created but its signal is never used when options.signal is provided. The 120-second timeout is silently bypassed, and the request can hang indefinitely until the caller aborts or the browser times out.",
    "impact": "Users may experience indefinitely hanging analysis requests if the AI proxy is slow or unresponsive, with no timeout protection. This wastes server resources and degrades user experience."
  },
  {
    "bug_id": "R09-C-012",
    "severity": "MEDIUM",
    "category": "Dependency Risk",
    "component": "Package Dependencies",
    "file_line": "package.json:23",
    "description": "Several dependencies use caret (^) version ranges, which means minor and patch versions can change between installs. Critical dependencies like firebase (^12.7.0), jspdf (^4.0.0), and exceljs (^4.4.0) could receive breaking updates within their major version. Additionally, there is no package-lock.json enforcement or lockfile integrity check in the build scripts, and the dependency count is large (17 runtime dependencies) with no evidence of periodic audit.",
    "steps": "1. Review package.json dependencies. 2. Run 'npm audit' to check for known vulnerabilities. 3. Note that caret ranges allow minor version drift. 4. Check if a lockfile is committed and if CI enforces 'npm ci' over 'npm install'.",
    "expected": "Production builds should use exact versions or a lockfile with integrity checks. The build script should use 'npm ci' to ensure reproducible builds. Regular dependency audits should be part of the CI pipeline.",
    "actual": "All dependencies use caret ranges. Build script uses 'vite build' without any lockfile integrity enforcement. No audit script is defined in package.json scripts.",
    "impact": "Non-reproducible builds across environments. Different developers or CI runs may get different dependency versions, leading to inconsistent behavior and hard-to-diagnose bugs. Undetected dependency vulnerabilities may persist."
  },
  {
    "bug_id": "R09-C-013",
    "severity": "MEDIUM",
    "category": "Caching Gap",
    "component": "Analysis Cache",
    "file_line": "src/lib/readability/utils/analysisCache.js:107",
    "description": "The analysis cache stores potentially large analysis result objects in localStorage (line 107), but there is no size check before writing. Each cached analysis contains checkResults, recommendations, llmExtractions, and aiAssessment data that could be several hundred KB when serialized to JSON. With MAX_CACHE_ENTRIES set to 20 (line 9), the cache could consume several MB of the ~5MB localStorage quota, directly competing with the storageHelpers.js STORAGE_KEYS data and potentially causing QuotaExceededError across the application.",
    "steps": "1. Run 20 different URL analyses to fill the cache. 2. Check localStorage usage via storageHelpers.checkStorageUsage(). 3. Observe that the readability cache entries are not counted by checkStorageUsage() because they use a different key prefix. 4. Attempt to save other data and observe potential quota exceeded errors.",
    "expected": "The analysis cache should coordinate with the storage management system in storageHelpers.js. Cache entries should have a size limit, and the total cache size should be bounded. checkStorageUsage() should account for readability cache entries.",
    "actual": "The analysis cache operates independently from storageHelpers.js storage management. It has no per-entry size limit and does not integrate with checkStorageUsage(), creating an invisible storage consumer that can exhaust the localStorage quota.",
    "impact": "Users with active readability analysis usage may experience data loss in other application features (timeline data, filter presets, time entries) when localStorage fills up, with no advance warning from the storage monitoring system."
  },
  {
    "bug_id": "R09-C-014",
    "severity": "MEDIUM",
    "category": "Monitoring Gap",
    "component": "Export History",
    "file_line": "src/hooks/useExportHistory.js:110",
    "description": "The logExport function (line 100-113) catches errors but only logs them to console.error and returns null. There is no metric tracking for export success/failure rates, no tracking of export durations, and no monitoring of which export types are most used. The deleteExport function (line 117-123) similarly only logs errors to console. For a business-critical feature like report generation and export, this lack of telemetry makes it impossible to understand export reliability and usage patterns.",
    "steps": "1. Trigger an export action. 2. Simulate a Firestore write failure. 3. Check for any monitoring or metrics that capture the failure. 4. Review the codebase for any export analytics or success rate tracking.",
    "expected": "Export operations should emit success/failure metrics to a monitoring service. Export type, format, duration, and file size should be tracked for operational visibility.",
    "actual": "Export failures are logged only to console.error. No metrics, analytics, or structured telemetry exists for export operations.",
    "impact": "Product and operations teams have no visibility into export reliability, usage patterns, or failure rates. Systematic export failures affecting users go undetected until users manually report issues."
  },
  {
    "bug_id": "R09-C-015",
    "severity": "LOW",
    "category": "Code Splitting",
    "component": "Vite Manual Chunks",
    "file_line": "vite.config.js:20",
    "description": "The manualChunks configuration in vite.config.js includes 'htmlparser2' and 'franc' in the 'vendor-readability' chunk (line 20), but these libraries are used exclusively by the readability analysis feature. Grouping them with jszip (which is used by both audit export and readability export) into a single vendor chunk means that navigating to any feature that uses jszip will also load htmlparser2 and franc, even if readability analysis is never used.",
    "steps": "1. Navigate to the audit export feature which uses jszip. 2. Check the network panel for loaded chunks. 3. Observe that htmlparser2 and franc are loaded alongside jszip even though they are not needed for audit export.",
    "expected": "htmlparser2 and franc should be in a separate chunk from jszip, or the readability-specific libraries should be loaded only when the readability feature is accessed.",
    "actual": "htmlparser2, franc, and jszip are bundled together in vendor-readability, causing unnecessary library loading for non-readability features that only need jszip.",
    "impact": "Minor increase in loaded JavaScript for users accessing audit features. Suboptimal code splitting reduces the effectiveness of lazy loading for the readability feature."
  },
  {
    "bug_id": "R09-C-016",
    "severity": "LOW",
    "category": "Error Logging",
    "component": "useProjects Hook",
    "file_line": "src/hooks/useProjects.js:90",
    "description": "The getProject function (lines 90-97) has no try-catch error handling. If the Firestore getDoc call fails due to a network error, permission issue, or invalid projectId, the error will bubble up as an unhandled promise rejection to the calling component. All other CRUD operations in the same hook (createProject, updateProject, deleteProject) have try-catch blocks, making this an inconsistent oversight.",
    "steps": "1. Call getProject() with an invalid or inaccessible project ID. 2. Observe that the unhandled error propagates to the component. 3. Compare with createProject/updateProject/deleteProject which all have error handling.",
    "expected": "getProject should have consistent error handling with try-catch, error logging, and a user-friendly error message, matching the pattern of the other CRUD operations in the same hook.",
    "actual": "getProject has no error handling. A Firestore error results in an unhandled promise rejection.",
    "impact": "Components calling getProject may crash without a helpful error message if the Firestore call fails, degrading user experience when viewing project details under degraded network conditions."
  }
]
