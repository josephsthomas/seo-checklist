[
  {
    "bug_id": "R11-C-001",
    "severity": "HIGH",
    "category": "Taxonomy Gap",
    "component": "Glossary Data",
    "file_line": "src/data/glossary.js:246",
    "description": "The glossaryCategories array includes categories that do not match the full set of categories actually assigned to glossary terms. The categories 'SERP Features', 'Link Building', and 'Content Quality' are used on individual terms (ids 8, 16, 7/30) but are missing from the glossaryCategories filter list. This means users filtering by category will never find those terms, creating orphaned taxonomy nodes.",
    "steps": "1. Open the glossary page. 2. Note the filter dropdown categories from glossaryCategories. 3. Compare with the category values on individual terms. 4. Observe 'SERP Features' (term id 8), 'Link Building' (term id 16), and 'Content Quality' (terms id 7, 30) are absent from the filter list.",
    "expected": "Every category value used on a glossary term should appear in the glossaryCategories array so that all terms are discoverable through category filtering.",
    "actual": "Three categories ('SERP Features', 'Link Building', 'Content Quality') are used on terms but omitted from the glossaryCategories filter array, making those terms unfilterable.",
    "impact": "Users attempting to filter glossary terms by category will not see terms filed under SERP Features, Link Building, or Content Quality, leading to incomplete taxonomy navigation and a false impression that those terms do not exist."
  },
  {
    "bug_id": "R11-C-002",
    "severity": "MEDIUM",
    "category": "Terminology",
    "component": "Glossary Data",
    "file_line": "src/data/glossary.js:31",
    "description": "The glossary entry for 'Core Web Vitals' (id 4) references 'INP (interactivity)' as the current interactivity metric, which is correct for WCAG 2.2 era. However, the Title Tag entry (id 26) contains 'Best Running Shoes 2024' in its example, which is a hardcoded year that becomes stale over time. The glossary lacks a dateLastReviewed or version metadata field on each term, meaning there is no governance mechanism to ensure terminology stays current.",
    "steps": "1. Open src/data/glossary.js. 2. Review the term entries for any temporal references. 3. Note the Title Tag example at line 211 references '2024'. 4. Note there is no dateLastReviewed, version, or lastUpdated field on any term.",
    "expected": "Glossary terms should include a dateLastReviewed or version metadata field to track freshness, and temporal references should be parameterized or flagged for periodic review.",
    "actual": "No version or review-date metadata exists on glossary terms, and at least one term contains a hardcoded year reference that will become stale.",
    "impact": "Without lifecycle metadata on glossary terms, there is no way to schedule or track content reviews, leading to progressively stale terminology that undermines the portal's authority as a reference tool."
  },
  {
    "bug_id": "R11-C-003",
    "severity": "CRITICAL",
    "category": "Content Model Issue",
    "component": "Checklist Data",
    "file_line": "src/data/checklistData.js:1",
    "description": "The checklist data model uses a flat array of 198+ items with inconsistent category taxonomy. The 'category' field uses free-text strings without a controlled vocabulary or enum. For example, 'Foundation & Setup', 'Technical SEO', 'Content Strategy', 'AI & Generative Search', 'On-Page Optimization', 'Schema Markup', etc. are all used but there is no authoritative category list exported alongside the data. Additionally, new categories like 'AI & Generative Search' appear only in later items (starting id 191) without being integrated into the earlier category structure, creating a disjointed content model.",
    "steps": "1. Open src/data/checklistData.js. 2. Grep for unique 'category' values across all items. 3. Note there is no exported CATEGORIES constant or enum. 4. Note 'AI & Generative Search' appears only from item 191 onward. 5. Compare with the FilterPresetManager and SEOChecklist components that must maintain their own category lists.",
    "expected": "The checklist data model should export a canonical CATEGORIES constant that enumerates all valid categories, and each checklist item should reference this controlled vocabulary to ensure taxonomy integrity.",
    "actual": "Categories are free-text strings with no controlled vocabulary, creating maintenance risk where UI filter components and data can drift out of sync.",
    "impact": "Filter and categorization features across the portal cannot reliably enumerate all categories, leading to potential data loss in filtering, broken faceted navigation, and difficulty maintaining taxonomy consistency as new items are added."
  },
  {
    "bug_id": "R11-C-004",
    "severity": "HIGH",
    "category": "Lifecycle Issue",
    "component": "Retention Policy",
    "file_line": "src/lib/retentionPolicy.js:6",
    "description": "The retention policy provides options for audit log retention (30, 90, 180, 365 days, unlimited) but lacks any content-specific retention policy. There is no retention or archiving strategy defined for the primary content types managed by the portal: readability analyses, schemas in the schema library, custom reports, checklist state, or project data. The useReadabilityAnalysis hook enforces per-role storage limits (line 38-44) but this is disconnected from the retention policy module, creating two independent lifecycle management systems.",
    "steps": "1. Open src/lib/retentionPolicy.js. 2. Note it only covers audit logs. 3. Open src/hooks/useReadabilityAnalysis.js line 38. 4. Note STORAGE_LIMITS is defined locally with no reference to the retention policy module. 5. Note no retention policy exists for schemas, reports, or project data.",
    "expected": "A unified content lifecycle policy should govern retention, archiving, and deletion across all content types (analyses, schemas, reports, projects, audit logs), with consistent configuration and a single source of truth.",
    "actual": "Retention is defined only for audit logs. Readability analyses have separate, disconnected storage limits hardcoded in a hook. Schemas, reports, and project data have no retention policy at all.",
    "impact": "Uncontrolled content growth in Firestore for schemas, reports, and projects could lead to storage cost issues and data governance violations. Inconsistent lifecycle management makes compliance reporting difficult."
  },
  {
    "bug_id": "R11-C-005",
    "severity": "HIGH",
    "category": "Governance Gap",
    "component": "Roles and Permissions",
    "file_line": "src/utils/roles.js:1",
    "description": "The role-permission model defines 6 roles with 10 permissions but lacks critical content governance permissions. There are no permissions for: canApproveContent, canPublish, canArchive, canDeleteContent, canManageGlossary, canManageTaxonomy, or canConfigureRetention. The CONTENT_WRITER role has identical permissions to DEVELOPER (lines 60-71 vs 48-59), which means there is no differentiation between these distinct professional roles. Additionally, the TASK_STATUS model (line 107) has only 4 states with no 'approved' or 'archived' state, missing critical editorial workflow stages.",
    "steps": "1. Open src/utils/roles.js. 2. Compare CONTENT_WRITER and DEVELOPER permissions - they are identical. 3. Note there is no canApproveContent, canPublish, or canArchive permission. 4. Note TASK_STATUS lacks 'approved', 'blocked', or 'archived' states.",
    "expected": "Content writer and developer roles should have differentiated permissions aligned with their responsibilities. The permission model should include content governance actions (approve, publish, archive). The task status model should include editorial workflow states.",
    "actual": "CONTENT_WRITER and DEVELOPER have identical permissions. No content governance permissions exist. TASK_STATUS lacks editorial workflow states like 'approved' or 'archived'.",
    "impact": "Without differentiated content governance permissions, there is no editorial approval workflow, no content publishing gate, and no role-based access control for sensitive content operations. This undermines content quality assurance processes."
  },
  {
    "bug_id": "R11-C-006",
    "severity": "MEDIUM",
    "category": "Content Reuse",
    "component": "Content Structure Checks / Reading Order Check",
    "file_line": "src/lib/readability/checks/contentStructure.js:148",
    "description": "The checkReadingOrder function (CS-09) duplicates nearly identical logic from checkHeadingHierarchy (CS-02). Both iterate over headings and check for level skips (current level > previous level + 1). CS-02 tracks detailed skip information while CS-09 counts issues. This violates the DRY principle and creates a content reuse problem where the same logic could diverge during maintenance. The two checks should share a common utility function.",
    "steps": "1. Open src/lib/readability/checks/contentStructure.js. 2. Compare checkHeadingHierarchy (lines 20-47) with checkReadingOrder (lines 148-172). 3. Note both iterate headings[i].level vs headings[i-1].level with the same condition (curr > prev + 1). 4. Note CS-09 produces a different output format but uses the same core detection logic.",
    "expected": "Common heading hierarchy analysis logic should be extracted into a shared utility function. CS-02 and CS-09 should consume the shared output and format their results differently.",
    "actual": "The heading hierarchy skip detection logic is duplicated across two check functions, creating maintenance risk and potential for inconsistent behavior.",
    "impact": "If the heading hierarchy detection logic is updated in one function but not the other, the two checks could produce contradictory results, confusing users and undermining confidence in the analysis tool."
  },
  {
    "bug_id": "R11-C-007",
    "severity": "MEDIUM",
    "category": "Metadata Issue",
    "component": "Schema Validator",
    "file_line": "src/lib/readability/utils/schemaValidator.js:6",
    "description": "The SCHEMA_FIELDS definitions for schema completeness scoring cover only 7 schema types (Article, NewsArticle, Product, FAQPage, HowTo, Organization, Person) while the SCHEMA_TYPES exported from useSchemaLibrary.js (line 178) lists 15 types including LocalBusiness, Event, Recipe, Review, BreadcrumbList, VideoObject, JobPosting, Course, and SoftwareApplication. This means the schema validator cannot provide detailed completeness scoring for over half the schema types that users can create and save in the library.",
    "steps": "1. Open src/lib/readability/utils/schemaValidator.js. 2. Note SCHEMA_FIELDS has 7 entries. 3. Open src/hooks/useSchemaLibrary.js line 178. 4. Note SCHEMA_TYPES has 15 entries. 5. Cross-reference - 8 schema types (LocalBusiness, Event, Recipe, Review, BreadcrumbList, VideoObject, JobPosting, Course, SoftwareApplication) have no validation spec.",
    "expected": "Every schema type offered in the schema library should have a corresponding completeness scoring specification in the schema validator, with required, recommended, and optional fields defined.",
    "actual": "8 of 15 supported schema types lack completeness scoring specifications, falling back to a generic field-counting heuristic that provides no actionable guidance on missing required or recommended fields.",
    "impact": "Users creating LocalBusiness, Event, Recipe, or other unsupported schema types receive generic, unhelpful completeness scores instead of specific guidance on which fields are required or recommended, reducing the tool's value for content optimization."
  },
  {
    "bug_id": "R11-C-008",
    "severity": "HIGH",
    "category": "Content Relationship",
    "component": "Recommendations Engine",
    "file_line": "src/lib/recommendations.js:64",
    "description": "The cross-tool recommendations engine defines recommendation rules that create a directional relationship graph between tools, but it has no 'readability' tool context. The TOOLS constant (line 7) lists 6 tools (planner, audit, accessibility, image-alt, meta-generator, schema-generator) but the readability checker - which is the most content-strategy-focused tool and generates rich actionable data - is completely absent. There are no recommendation rules that connect readability analysis results to other tools, and no rules that recommend readability analysis from other tool contexts.",
    "steps": "1. Open src/lib/recommendations.js. 2. Examine the TOOLS constant - no 'readability' entry exists. 3. Examine RECOMMENDATION_RULES - no rule has context 'readability' and no rule recommends 'readability'. 4. Note the readability checker generates scores across metadata, content structure, and AI signals that could drive recommendations to meta-generator, schema-generator, and image-alt tools.",
    "expected": "The readability checker should be included in the TOOLS registry and have bidirectional recommendation rules connecting it to other tools based on analysis results (e.g., low metadata score -> recommend meta-generator).",
    "actual": "The readability checker is entirely absent from the recommendations engine, breaking the content relationship model and preventing intelligent cross-tool workflows.",
    "impact": "Users completing a readability analysis receive no guidance on which other tools to use to address identified issues, breaking the portal's workflow integration story and reducing tool adoption across the suite."
  },
  {
    "bug_id": "R11-C-009",
    "severity": "MEDIUM",
    "category": "Taxonomy Gap",
    "component": "Industry Profiles",
    "file_line": "src/lib/readability/profiles/industryProfiles.js:6",
    "description": "The industry profiles system defines only 5 profiles (technology, healthcare, ecommerce, media, legal) but the profile weight overrides reference only 5 scoring categories. The profiles lack coverage for major industry verticals that have distinct content strategy requirements: education, government, non-profit, real estate, travel/hospitality, and manufacturing. More critically, the profiles do not include a 'general' or 'default' fallback profile, meaning users who don't select a profile get no industry-specific guidance at all.",
    "steps": "1. Open src/lib/readability/profiles/industryProfiles.js. 2. Note only 5 industry profiles exist. 3. Call getIndustryProfile with any key not in the list (e.g., 'education'). 4. It returns null with no fallback. 5. Note getAvailableProfiles() returns only 5 options.",
    "expected": "The industry profiles should include at minimum a 'general/default' profile and cover the most common industry verticals. The getIndustryProfile function should return a sensible default when the requested profile is not found.",
    "actual": "Only 5 specialized profiles exist with no fallback. getIndustryProfile returns null for unrecognized keys, meaning callers must handle null defensively or risk errors.",
    "impact": "Users in education, government, non-profit, or other unlisted industries get no industry-specific content optimization guidance, reducing the tool's relevance and value for a significant portion of the user base."
  },
  {
    "bug_id": "R11-C-010",
    "severity": "MEDIUM",
    "category": "Content Model Issue",
    "component": "Fact Density Check",
    "file_line": "src/lib/readability/checks/factDensity.js:40",
    "description": "The checkFactDensity function outputs a result with category value 'contentClarity' (lowercase camelCase, line 48) while the parent contentClarity check module outputs results with category 'Content Clarity' (title case with space). This inconsistency in the content model means that any downstream consumer grouping or filtering results by category will treat fact density as a separate category from the other Content Clarity checks. The scorer.js pushes factDensityResult into contentClarityResults (line 35), but the result's own category metadata is inconsistent.",
    "steps": "1. Open src/lib/readability/checks/factDensity.js line 48. 2. Note category is 'contentClarity'. 3. Open src/lib/readability/checks/contentClarity.js line 8. 4. Note CATEGORY is 'Content Clarity'. 5. In scorer.js line 35, factDensityResult is pushed into contentClarityResults. 6. The result's category field does not match the other items in the array.",
    "expected": "All checks within a category should use the same category string value. The factDensity check should use 'Content Clarity' to match the other content clarity checks.",
    "actual": "factDensity uses 'contentClarity' while all other Content Clarity checks use 'Content Clarity', creating an inconsistent content model.",
    "impact": "Any UI component that groups or displays checks by their category field will show fact density separately from other Content Clarity checks, creating a confusing and fragmented user experience in the dashboard."
  },
  {
    "bug_id": "R11-C-011",
    "severity": "HIGH",
    "category": "Editorial Workflow",
    "component": "Due Dates / Content Lifecycle",
    "file_line": "src/hooks/useDueDates.js:64",
    "description": "The due date system supports only three type values: 'task', 'project', and 'reminder' (line 64). There is no support for content lifecycle milestone types such as 'content_review', 'content_refresh', 'content_archive', 'content_publish', or 'content_approval'. This means the editorial workflow cannot track content-specific lifecycle events like scheduled content reviews, publication deadlines, or archival dates. The due date urgency calculation (getDueDateUrgency, line 177) also uses a generic color system with 'charcoal' for future items, which is not a standard Tailwind CSS color and may not render correctly.",
    "steps": "1. Open src/hooks/useDueDates.js. 2. Note line 64 defines only 3 types: task, project, reminder. 3. Attempt to create a due date for a content review cycle - no appropriate type exists. 4. Check getDueDateUrgency line 186 - 'charcoal' is not a standard Tailwind color.",
    "expected": "The due date type taxonomy should include content lifecycle event types (content_review, content_publish, content_archive) to support editorial workflows. Urgency colors should use valid Tailwind CSS color names.",
    "actual": "Only generic types (task, project, reminder) are supported. No content-specific lifecycle event types exist. The 'charcoal' color may not render in Tailwind.",
    "impact": "Content teams cannot schedule or track content lifecycle events through the due date system, forcing them to use generic 'task' or 'reminder' types that provide no content-specific context or reporting capability."
  },
  {
    "bug_id": "R11-C-012",
    "severity": "MEDIUM",
    "category": "Content Model Issue",
    "component": "Google AIO Checks",
    "file_line": "src/lib/readability/checks/googleAIO.js:48",
    "description": "The Google AIO checks module defines 5 checks (AIO-01 through AIO-05) with category value 'aiSignals' (camelCase) while the main AI Signals check module uses 'AI-Specific Signals' as its category string. The scorer.js (line 36) appends Google AIO results into aiSignalsResults, but the category metadata on each AIO check result does not match the parent category name. Additionally, AIO checks use a 'description' field (e.g., line 33) while the standard check format uses 'recommendation', creating an inconsistent content model across check result objects.",
    "steps": "1. Open src/lib/readability/checks/googleAIO.js. 2. Note checks use category: 'aiSignals' and include a 'description' field. 3. Open src/lib/readability/checks/aiSignals.js line 8. 4. Note CATEGORY is 'AI-Specific Signals' and checks use 'recommendation' not 'description'. 5. In scorer.js line 36, AIO checks are pushed into aiSignalsResults but have different field structure.",
    "expected": "All checks appended to a category array should use the same category string and the same result object schema (either 'recommendation' or 'description', not both).",
    "actual": "Google AIO checks use 'aiSignals' category and 'description' field, while the parent AI Signals checks use 'AI-Specific Signals' category and 'recommendation' field. This creates two incompatible result schemas in the same array.",
    "impact": "UI components rendering check results cannot consistently access recommendation text - some checks have it in 'recommendation', others in 'description'. Category-based grouping and filtering will treat AIO checks as a separate category."
  },
  {
    "bug_id": "R11-C-013",
    "severity": "MEDIUM",
    "category": "Governance Gap",
    "component": "Schema Library",
    "file_line": "src/hooks/useSchemaLibrary.js:75",
    "description": "The schema library allows users to mark schemas as public (isPublic: true, line 75) but there is no governance mechanism to review or approve publicly shared schemas. Any user can make a schema public without editorial review, and there is no moderation, version control, or quality gate. The deleteSchema function (line 108) also lacks ownership verification - it does not check if the current user owns the schema before deletion, unlike saveSchema which requires authentication.",
    "steps": "1. Open src/hooks/useSchemaLibrary.js. 2. Note saveSchema allows isPublic flag without review (line 75). 3. Note deleteSchema has no ownership check (line 108-119). 4. Note updateSchema also has no ownership check (line 91-105). 5. Observe there is no approval workflow for making schemas public.",
    "expected": "Public schema sharing should require editorial review or admin approval. Delete and update operations should verify ownership. A version history should be maintained for shared schemas.",
    "actual": "Any authenticated user can make schemas public without review. Delete and update operations lack ownership verification. No version history exists.",
    "impact": "Without governance on public schemas, users could share incorrect or misleading structured data templates that other users adopt, potentially harming their SEO. Lack of ownership checks could lead to accidental or malicious deletion of other users' schemas."
  },
  {
    "bug_id": "R11-C-014",
    "severity": "LOW",
    "category": "Metadata Issue",
    "component": "Technical Access Check - Robots.txt",
    "file_line": "src/lib/readability/checks/technicalAccess.js:52",
    "description": "The checkRobotsTxt function (TA-03) always returns status 'warn' regardless of the analysis outcome. Line 59 has a conditional that evaluates to 'warn' in both the true and false branches: `status: crawlerIssues.length > 0 ? 'warn' : 'warn'`. This means the check can never pass or fail, always producing a warning that adds noise to the results without actionable differentiation.",
    "steps": "1. Open src/lib/readability/checks/technicalAccess.js line 59. 2. Note the ternary: `crawlerIssues.length > 0 ? 'warn' : 'warn'`. 3. Both branches return 'warn'. 4. Run the check with any content - it will always show as a warning.",
    "expected": "The status should differentiate between states: 'pass' when no AI-specific meta directives are found, 'warn' when directives are informational, and 'fail' when restrictive directives are detected.",
    "actual": "The function returns 'warn' in both ternary branches, making the conditional meaningless and the check permanently inconclusive.",
    "impact": "Every page analyzed will show a robots.txt warning regardless of actual status, contributing to alert fatigue and making it impossible for users to distinguish pages with genuine robots.txt issues from pages without them."
  },
  {
    "bug_id": "R11-C-015",
    "severity": "MEDIUM",
    "category": "Content Relationship",
    "component": "Scorer / Category Weight Configuration",
    "file_line": "src/lib/readability/scorer.js:84",
    "description": "The scorer outputs category weights as hardcoded display strings (e.g., weight: '20%' on line 84) that are disconnected from the actual calculation weights in scoreCalculator.js (CATEGORY_WEIGHTS). The scorer claims 'Content Structure: 20%, Content Clarity: 25%, Technical Accessibility: 20%, Metadata & Schema: 15%, AI-Specific Signals: 20%' which totals 100%. However, if industry profiles override these weights (industryProfiles.js), the display weights will be wrong because they are static strings, not computed from the actual weights used. The content relationship between displayed weights and computed weights is broken.",
    "steps": "1. Open src/lib/readability/scorer.js lines 83-89. 2. Note weights are hardcoded strings. 3. Open src/lib/readability/utils/scoreCalculator.js lines 6-12. 4. Note actual calculation weights. 5. Open src/lib/readability/profiles/industryProfiles.js. 6. Note industry profiles define weightOverrides (e.g., technology: contentStructure: 25). 7. When an industry profile is applied, the displayed weight strings will not reflect the overridden values.",
    "expected": "Display weights should be dynamically computed from the actual weights used in scoring, including any industry profile overrides, to maintain accuracy between what is shown and what is calculated.",
    "actual": "Display weights are hardcoded strings that do not reflect industry profile weight overrides, creating a misleading representation of how scores are computed.",
    "impact": "Users who select an industry profile will see incorrect weight percentages in the dashboard, undermining trust in the scoring methodology and making it impossible to understand how their score was actually calculated."
  }
]
